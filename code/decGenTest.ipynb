{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0b1e6-21e5-408f-90bf-c1d184bf5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Decoy generation workflow\"\"\"\n",
    "\n",
    "# Import relevant packages\n",
    "from CGRtools.files import RDFRead, RDFWrite\n",
    "from CGRtools.exceptions import *\n",
    "# from ..util.utils import (generate_reactions, remove_reagents,\n",
    "#                           containers_split, not_radical,\n",
    "#                           get_rules)\n",
    "# from ..util.routine import (_save_log, _util_file)\n",
    "from datetime import date\n",
    "\n",
    "import os\n",
    "import math\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "\n",
    "def main(n_proc):\n",
    "    \"\"\"\n",
    "    Main generation routine\n",
    "\n",
    "    =======================   ===================================\n",
    "    Reactions in input file   1 316 541\n",
    "    Data                      reactions, type: ReactionContainer\n",
    "    =======================   ===================================\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    :param n_proc: Num pool worker\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    with open(\"Config.pickle\", \"rb\") as configFile:\n",
    "        config_list = pickle.load(configFile)\n",
    "        name_in, templates_fn, name_out, log_filename, batch, max_decoys, limit, v, log = config_list\n",
    "\n",
    "    with open(templates_fn, \"rb\") as pkl:\n",
    "        templates = pickle.load(pkl)\n",
    "\n",
    "    with RDFRead(name_in, indexable=True) as data, \\\n",
    "    open(name_out, \"a\") as w, RDFWrite(w) as rdf, \\\n",
    "    open(\"NonRecon_smiles.rdf\", \"a\") as z, RDFWrite(z) as nrdf, \\\n",
    "    open(\"Removed_smiles.rdf\", \"a\") as y, RDFWrite(y) as rmvd:\n",
    "\n",
    "        name = \"Worker-{}\".format(n_proc)\n",
    "        id_start = n_proc * batch\n",
    "        id_stop = id_start + batch\n",
    "\n",
    "        # LOGGING\n",
    "        if log:\n",
    "            _save_log(log_filename, str(\"Process {} initialized\\n\".format(name)))\n",
    "        start_time = time.time()\n",
    "\n",
    "        for reaction in data[id_start: id_stop]:\n",
    "            doc = {}\n",
    "            reaction = remove_reagents(reaction)\n",
    "            if reaction is not None:\n",
    "                reaction = containers_split(reaction)\n",
    "                if len(reaction.reactants) == 2:\n",
    "                    cgr = reaction.compose()\n",
    "                    if not_radical(cgr):\n",
    "                        if db_check(cgr): # the num of dyn bonds in a CGR must be > 1 and < or = 7\n",
    "                            reaction.meta.update({\"type\": \"Initial\"})\n",
    "                            doc.update({str(reaction): {\"structure\": reaction,\n",
    "                                                        \"type\": reaction.meta[\"type\"]}})\n",
    "\n",
    "                            doc = generate_reactions(reaction, reaction.reactants, templates,\n",
    "                                                     max_decoys, limit, doc)\n",
    "\n",
    "                            if any([True if inner_dict[\"type\"] == \"Reconstructed\" else False for inner_dict in\n",
    "                                    doc.values()]):\n",
    "                                if v:\n",
    "                                    print(\"REC;ID:{};NUM:{}\\n\".format(\n",
    "                                            reaction.meta[\"Reaction_ID\"], len(doc)))\n",
    "                                if log:\n",
    "                                    _save_log(log_filename,\n",
    "                                              str(\"REC;ID:{};NUM:{}\\n\".format(\n",
    "                                                  reaction.meta[\"Reaction_ID\"], len(doc))))\n",
    "                                for rxn in [rxn[\"structure\"] for rxn in doc.values()]:\n",
    "                                    rdf.write(rxn)\n",
    "                            else:\n",
    "                                if v:\n",
    "                                    print(\"NREC;ID:{};NUM:{}\\n\".format(reaction.meta[\"Reaction_ID\"], len(doc)))\n",
    "                                if log:\n",
    "                                    _save_log(log_filename,\n",
    "                                              str(\"NREC;ID:{};NUM:{}\\n\".format(\n",
    "                                                  reaction.meta[\"Reaction_ID\"], len(doc))))\n",
    "                                for rxn in [rxn[\"structure\"] for rxn in doc.values()]:\n",
    "                                    nrdf.write(rxn)\n",
    "                        else:\n",
    "                            reaction.meta.update({\"removed\": \"dynbonds\"})\n",
    "                            rmvd.write(reaction)\n",
    "                    else:\n",
    "                        reaction.meta.update({\"removed\": \"radical\"})\n",
    "                        rmvd.write(reaction)\n",
    "                else:\n",
    "                    reaction.meta.update({\"removed\": \"nonbinary\"})\n",
    "                    rmvd.write(reaction)\n",
    "        else:\n",
    "            end_time = time.time()\n",
    "            if v:\n",
    "                print(\"Process {} finished batch processing in time: {}s\\n\".format(name, end_time - start_time))\n",
    "            if log:\n",
    "                _save_log(log_filename,\n",
    "                          str(\"Process {} finished batch processing in time: {}s\\n\".format(name,\n",
    "                                                                                           end_time - start_time)))\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    name_in = \"/home/vendinm/caspFilter/data/decoyGeneration/test_folder/TEST.rdf\"\n",
    "    template_pkl = \"/home/vendinm/caspFilter/data/templateGeneration/caspRules2022-02-26.pickle\"\n",
    "    num_proc = 25\n",
    "    log_filename = \"GENERATE_DECOYS_LOG_SMILES.txt\" # smth\n",
    "    name_out = 'test_smiles_{}.rdf'.format(date.today())\n",
    "    batch = 10000\n",
    "    v = False\n",
    "    num = 50\n",
    "    lim = 10\n",
    "    log = True\n",
    "    \n",
    "    _util_file(\"NonRecon_smiles.rdf\")  # deleting the rdf files if it was created earlier in the directory\n",
    "    _util_file(\"Removed_smiles.rdf\")\n",
    "    _util_file(name_out)\n",
    "    if log: \n",
    "        _util_file(log_filename)\n",
    "    \n",
    "    with open(\"Config.pickle\", \"wb\") as config:\n",
    "        config_list = [\n",
    "            str(name_in),\n",
    "            str(template_pkl),\n",
    "            str(name_out),\n",
    "            str(log_filename),\n",
    "            int(batch),\n",
    "            int(num),\n",
    "            int(lim),\n",
    "            bool(v),\n",
    "            bool(log)\n",
    "        ]\n",
    "        pickle.dump(config_list, config)\n",
    "\n",
    "    with RDFRead(name_in, indexable=True) as file:\n",
    "        chunk = math.ceil(len(file) // 10000)\n",
    "        \n",
    "    with multiprocessing.Pool(processes=num_proc) as pool:\n",
    "        pool.map(main, list(range(chunk)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba656d29-f94f-4f72-933d-4e1570239ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some routines for generation workflow\"\"\"\n",
    "# from routine import (_remove_mols, _db_check)\n",
    "from CGRtools.reactor import Reactor\n",
    "from CGRtools.containers import ReactionContainer\n",
    "from CGRtools.exceptions import (InvalidAromaticRing,\n",
    "                                 MappingError)\n",
    "\n",
    "\n",
    "def remove_reagents(reaction):\n",
    "    \"\"\"\n",
    "    Removing unchanging molecules, and checking reaction properties\n",
    "    :param reaction: input reaction\n",
    "    :return: ReactionContainer or None in some cases\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cgr = reaction.compose()\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "    to_remove = []\n",
    "    for n, i in enumerate(cgr.split()): # remove unchanging molecules\n",
    "        if any([z[1].charge != z[1].p_charge for z in i.atoms()]) or \\\n",
    "           any([z[2].order != z[2].p_order for z in i.bonds()]):\n",
    "            continue\n",
    "        to_remove.extend([x for x, _ in i.atoms()])\n",
    "    for mol in reaction.molecules():\n",
    "        for z in to_remove:\n",
    "            try:\n",
    "                mol.delete_atom(z)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "    try:\n",
    "        reaction.canonicalize()\n",
    "    except InvalidAromaticRing:\n",
    "        return None\n",
    "    reaction.flush_cache()\n",
    "    reactants = [x for x in reaction.reactants if x]\n",
    "    products = [x for x in reaction.products if x]\n",
    "    if len(reactants) > 0 and len(products) > 0:\n",
    "        reaction = ReactionContainer(reactants=reactants,\n",
    "                                     products=products,\n",
    "                                     meta=reaction.meta)\n",
    "        return reaction\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def db_check(cgr):\n",
    "    if len(cgr.center_bonds) > 1 and len(cgr.center_bonds) <= 7:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def not_radical(cgr):\n",
    "    \"\"\"\n",
    "    Checking for charged atoms in a Condensed Graph of Reaction\n",
    "    :param cgr: Condensed Graph of the input reaction\n",
    "    :return: bool\n",
    "    \"\"\"\n",
    "    if cgr and cgr.center_atoms:\n",
    "        if any(x.is_radical or x.p_is_radical for _, x in cgr.atoms()):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def containers_split(reaction):\n",
    "    \"\"\"\n",
    "    Separation of MoleculeContainers\n",
    "    :param reaction: input reaction\n",
    "    :return: ReactionContainer\n",
    "    \"\"\"\n",
    "    new_reactants = [x for container in reaction.reactants for x in container.split()]\n",
    "    new_products = [x for container in reaction.products for x in container.split()]\n",
    "    return ReactionContainer(reactants=new_reactants,\n",
    "                             products=new_products,\n",
    "                             meta=reaction.meta)\n",
    "\n",
    "\n",
    "def generate_reactions(reaction, reactants, templates, max_decoys, limit, doc):\n",
    "    \"\"\"\n",
    "    Accumulation of generated reactions\n",
    "    :param reaction: input reaction\n",
    "    :param reactants: reactants of input reaction\n",
    "    :param templates: template list of reaction transformations (list[ReactionContainer, ...])\n",
    "    :param max_decoys: max number of reaction to generate\n",
    "    :param limit: max number of reaction from one transformation\n",
    "    :param doc: Dict[..., Dict[...]}]\n",
    "    \"\"\"\n",
    "    for rxn in apply_templates(reaction, reactants, templates, limit):\n",
    "        if len(doc) == max_decoys:\n",
    "            break\n",
    "\n",
    "        new_reaction = ReactionContainer(reactants=reactants,\n",
    "                                         products=rxn.products,\n",
    "                                         meta=rxn.meta)\n",
    "        new_reaction.meta.update(reaction.meta)\n",
    "        try:\n",
    "            new_reaction = containers_split(new_reaction)\n",
    "            new_reaction.canonicalize()\n",
    "        except InvalidAromaticRing:\n",
    "            continue\n",
    "        try:\n",
    "            if (new_reaction.compose()).center_atoms:\n",
    "                try:\n",
    "                    if str(new_reaction) not in doc:\n",
    "                        new_reaction.meta.update({\"type\": \"Decoy\"})\n",
    "                    elif str(new_reaction) in doc and \\\n",
    "                        doc[str(new_reaction)][\"type\"] == \"Initial\":\n",
    "                        new_reaction.meta.update({\"type\": \"Reconstructed\"})\n",
    "                    else:\n",
    "                        continue\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                doc.update({str(new_reaction): {\"structure\": new_reaction,\n",
    "                                                \"type\": new_reaction.meta[\"type\"]}})\n",
    "            else:\n",
    "                continue\n",
    "        except MappingError:\n",
    "            continue\n",
    "    return doc\n",
    "\n",
    "\n",
    "def apply_templates(reaction, reactants, templates, limit):\n",
    "    \"\"\"\n",
    "    New reaction generator\n",
    "    :param reactants: reactants of input reaction\n",
    "    :param templates: template list of reaction transformations (list[ReactionContainer, ...])\n",
    "    :param limit: max number of reaction from one transformation\n",
    "    :return: yield(ReactionContainer) of generated reaction\n",
    "    \"\"\"\n",
    "    strict_templates = get_rules(reaction)\n",
    "    \n",
    "    for template in strict_templates:\n",
    "        try:\n",
    "            templates.remove(template)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        templates.insert(0, template)\n",
    "    \n",
    "    reactors = [Reactor(template,\n",
    "                        delete_atoms=True,\n",
    "                        one_shot=True,\n",
    "                        automorphism_filter=False) for template in templates]  # NB! CGRtools v. > 4.1.22\n",
    "    queue = [r(reactants) for r in reactors]\n",
    "    seen = set()\n",
    "    while queue:\n",
    "        reactor_call = queue.pop(0)\n",
    "        try:\n",
    "            num = 0\n",
    "            for new_reaction in reactor_call:\n",
    "                if num + 1 == limit:\n",
    "                    break\n",
    "                if str(new_reaction) not in seen:\n",
    "                    seen.add(str(new_reaction))\n",
    "                    num += 1\n",
    "                    yield new_reaction\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "def get_strict_templates(rxn_id, templates):\n",
    "    template_set = []\n",
    "    for template in templates:\n",
    "        if rxn_id in template.meta[\"id_in_rdf\"]:\n",
    "            template_set.append(template)\n",
    "    return template_set\n",
    "\n",
    "\n",
    "def get_rules(reaction):\n",
    "    \"\"\"\n",
    "    Obtaining the rules of reaction transformations\n",
    "    NB! CGRtools.enumerate_centers() is used\n",
    "    :param reaction: input reaction\n",
    "    :return: list[ReactionContainer, ...]\n",
    "    \"\"\"\n",
    "    reaction.clean_stereo()\n",
    "    rules = []\n",
    "\n",
    "    for n, partial_reaction in enumerate(reaction.enumerate_centers()):  # getting single stages\n",
    "        if n == 5:\n",
    "            break  # without an endless loop may appear, cause of many reaction centers\n",
    "        for reaction_center in partial_reaction.extended_centers_list:\n",
    "\n",
    "            reactants = reaction.reactants\n",
    "            products = reaction.products\n",
    "\n",
    "            cleavage = set(partial_reaction.reactants).difference(partial_reaction.products)\n",
    "            coming = set(partial_reaction.products).difference(partial_reaction.reactants)\n",
    "\n",
    "            bare_reaction_center = set(partial_reaction.compose().center_atoms)\n",
    "            \n",
    "            rule_reac = [x.substructure(set(reaction_center).intersection(x),\n",
    "                                        as_query=True) for x in reactants if set(reaction_center).intersection(x)]\n",
    "            rule_prod = [x.substructure(set(reaction_center).intersection(x),\n",
    "                                        as_query=True) for x in products if set(reaction_center).intersection(x)]\n",
    "\n",
    "            if len(rule_reac) != 2:\n",
    "                continue\n",
    "\n",
    "            rule = ReactionContainer(reactants=rule_reac,\n",
    "                                     products=rule_prod,\n",
    "                                     meta=reaction.meta)\n",
    "\n",
    "            for molecule in rule.molecules():\n",
    "                molecule._rings_sizes = {x: () for x in molecule._rings_sizes}  # getting rid of the ring sizes info (NEC.)\n",
    "                molecule._hydrogens = {x: () for x in molecule._hydrogens}  # getting rid of the hydrogen info (NEC.)\n",
    "\n",
    "            for at_env in set(reaction_center).difference(cleavage.union(coming).union(bare_reaction_center)):\n",
    "\n",
    "                for x in rule.reactants:\n",
    "                    if at_env in x.atoms_numbers:\n",
    "                        x._neighbors[at_env] = ()\n",
    "\n",
    "                for x in rule.products:\n",
    "                    if at_env in x.atoms_numbers:\n",
    "                        x._neighbors[at_env] = ()\n",
    "\n",
    "            for del_hyb_atom in cleavage:\n",
    "                for molecule in rule.reactants:\n",
    "                    if del_hyb_atom in molecule:\n",
    "                        molecule._hybridization[del_hyb_atom] = ()  # getting rid of the hybridization info in react. (NEC.)\n",
    "\n",
    "            for del_hyb_atom in coming:\n",
    "                for molecule in rule.products:\n",
    "                    if del_hyb_atom in molecule:\n",
    "                        molecule._hybridization[del_hyb_atom] = ()  # getting rid of the hybridization info in prod. (NEC.)\n",
    "\n",
    "            rule.flush_cache()\n",
    "            rules.append(rule)\n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf2dbb5-3f27-4c97-8912-949572c91d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Routine functions\"\"\"\n",
    "from CGRtools.files import (RDFRead, RDFWrite)\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "def _load_pkl(filename, n):\n",
    "    with open(\"{}_{}.pickle\".format(filename, n), \"rb\") as f:\n",
    "        for i in range(2 ** 64):\n",
    "            try:\n",
    "                yield pickle.load(f)\n",
    "            except EOFError:\n",
    "                break\n",
    "\n",
    "\n",
    "def _dump_pkl(filename, to_save, n):\n",
    "    with open(\"{}_{}.pickle\".format(filename, n), \"ab\") as f:\n",
    "        pickle.dump(to_save, f)\n",
    "    return\n",
    "\n",
    "\n",
    "def _util_file(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def _save_log(filename, string):\n",
    "    with open(filename, \"a\") as flog:\n",
    "        flog.write(string)\n",
    "\n",
    "\n",
    "def RDFclean(RDFfilename, log, dump_size, dump_fn, v):\n",
    "    \"\"\"\n",
    "    Dump pickling and removing duplicates\n",
    "    :param RDFfilename: RDF file to check duplicates and examine\n",
    "    :param log: log if necessary\n",
    "    :param dump_size: size of dict to dump\n",
    "    :param dump_fn: outer file name\n",
    "    :param v: printing if necessary\n",
    "    \"\"\"\n",
    "    with RDFRead(RDFfilename, indexable=True) as file:\n",
    "\n",
    "        to_save = dict()\n",
    "        log_filename = \"CLEANING_LOG.txt\"\n",
    "        flag = True\n",
    "        flag_pass = True\n",
    "\n",
    "        for n, reaction in enumerate(tqdm(file), start=1):\n",
    "            if n != 2210001 and flag_pass:\n",
    "                continue\n",
    "            else:\n",
    "                flag_pass = False\n",
    "            if flag:\n",
    "                num = n - 1\n",
    "                flag = False\n",
    "            try:\n",
    "                if str(reaction.compose()) not in to_save:\n",
    "                    to_save.update({str(reaction.compose()): reaction})\n",
    "                else:\n",
    "                    if reaction.meta[\"type\"].startswith(\"Reconstructed\"):\n",
    "                        if v: print(\"Replaced by reconstructed: {}\".format(reaction.meta[\"Reaction_ID\"]))\n",
    "                        if log: _save_log(log_filename,\n",
    "                                          str(\"Replaced by reconstructed: {}\\n\".format(reaction.meta[\"Reaction_ID\"])))\n",
    "                        del to_save[str(reaction.compose())]\n",
    "                        to_save.update({str(reaction.compose()): reaction})\n",
    "                    elif reaction.meta[\"type\"].startswith(\"Decoy\"):\n",
    "                        if v: print(\"Founded duplicate decoy: {}, real id: {}\".format(reaction.meta[\"Reaction_ID\"],\n",
    "                                                                                      to_save[\n",
    "                                                                                          str(reaction.compose())].meta[\n",
    "                                                                                          \"Reaction_ID\"]))\n",
    "                        if log: _save_log(log_filename, str(\"Founded duplicate decoy: {}, real id: {}\\n\".format(\n",
    "                            reaction.meta[\"Reaction_ID\"],\n",
    "                            to_save[str(reaction.compose())].meta[\"Reaction_ID\"])))\n",
    "                        continue\n",
    "            except Exception as e:\n",
    "                if v: print(\"{} was occurred, number: {}, rxn_ID: {}\\n\".format(e, n, reaction.meta[\"Reaction_ID\"]))\n",
    "                if log: _save_log(log_filename, str(\"{} was occurred, number: {}, rxn_ID: {}\\n\".format(e, n,\n",
    "                                                                                                      reaction.meta[\n",
    "                                                                                                          \"Reaction_ID\"]\n",
    "                                                                                                       )))\n",
    "                continue\n",
    "            if n % dump_size == 0:\n",
    "                flag = True\n",
    "                try:\n",
    "                    for rxn_dict in _load_pkl(dump_fn, num):\n",
    "                        for duplicate in set(to_save).intersection(set(rxn_dict)):\n",
    "                            try:\n",
    "                                if to_save[duplicate].meta[\"type\"].startswith(\"Reconstructed\"):\n",
    "                                    del rxn_dict[duplicate]\n",
    "                                elif to_save[duplicate].meta[\"type\"].startswith(\"Decoy\"):\n",
    "                                    del to_save[duplicate]\n",
    "                            except KeyError:\n",
    "                                del to_save[duplicate]\n",
    "                                continue\n",
    "                        _dump_pkl(dump_fn, rxn_dict, n)\n",
    "                    else:\n",
    "                        _dump_pkl(dump_fn, to_save, n)\n",
    "                        to_save = dict()\n",
    "                        _util_file(\"{}_{}.pickle\".format(dump_fn, num))\n",
    "                except (FileNotFoundError, UnboundLocalError):\n",
    "                    _dump_pkl(dump_fn, to_save, n)\n",
    "                    to_save = dict()\n",
    "        else:\n",
    "            for rxn_dict in _load_pkl(dump_fn, num):\n",
    "                for duplicate in set(to_save).intersection(set(rxn_dict)):\n",
    "                    try:\n",
    "                        if to_save[duplicate].meta[\"type\"].startswith(\"Reconstructed\"):\n",
    "                            del rxn_dict[duplicate]\n",
    "                        elif to_save[duplicate].meta[\"type\"].startswith(\"Decoy\"):\n",
    "                            del to_save[duplicate]\n",
    "                    except KeyError:\n",
    "                        del to_save[duplicate]\n",
    "                        continue\n",
    "                _dump_pkl(dump_fn, rxn_dict, n)\n",
    "            else:\n",
    "                _dump_pkl(dump_fn, to_save, n)\n",
    "                _util_file(\"{}_{}.pickle\".format(dump_fn, num))\n",
    "\n",
    "def Compile(input_file, output_file):\n",
    "    with open(input_file, \"rb\") as file, \\\n",
    "            open(output_file, \"ab\") as new_file, \\\n",
    "            open(\"Nonrecon.pickle\", \"ab\") as Nonrec_file:\n",
    "        counter = {}\n",
    "        len_list = []\n",
    "        nonrec_counter = {}\n",
    "        num_examined = 0\n",
    "        id_len = 0\n",
    "        init = \"Not found yet\"\n",
    "\n",
    "        for i in range(2 ** 64):\n",
    "            try:\n",
    "                data = pickle.load(file)\n",
    "                len_list.append(len(data))\n",
    "                if i == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    id_len += 1\n",
    "            except EOFError:\n",
    "                break\n",
    "\n",
    "            for n, reaction in enumerate(tqdm(data.values())):\n",
    "                num_examined += 1\n",
    "                try:\n",
    "                    if reaction.meta[\"Reaction_ID\"] != init:\n",
    "                        try:\n",
    "                            if counter[init][\"Recon_str\"] != 0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                nonrec_counter.update({init: counter[init]})\n",
    "                                del counter[init]\n",
    "                        except KeyError:\n",
    "                            print(\"KeyError occurred: {}\".format(reaction.meta[\"Reaction_ID\"]))\n",
    "                            pass\n",
    "\n",
    "                        if num_examined >= len_list[id_len - 1]:\n",
    "                            pickle.dump(counter, new_file)\n",
    "                            pickle.dump(nonrec_counter, Nonrec_file)\n",
    "                            num_examined = 0\n",
    "                            counter = {}\n",
    "                            nonrec_counter = {}\n",
    "\n",
    "                        init = reaction.meta[\"Reaction_ID\"]\n",
    "                        num = 0\n",
    "                        num_random = 0\n",
    "                        num_strict = 0\n",
    "                        structures = []\n",
    "                        recon_str = 0\n",
    "                        rand_ids = []\n",
    "                    structures.append(reaction)\n",
    "                    num += 1\n",
    "                    if not reaction.meta[\"type\"].startswith(\"Reconstructed\"):\n",
    "                        if \"Rule_ID\" in reaction.meta:\n",
    "                            num_random += 1\n",
    "                            rand_ids.append(reaction.meta[\"Rule_ID\"])\n",
    "                        else:\n",
    "                            num_strict += 1\n",
    "                    else:\n",
    "                        recon_str = reaction\n",
    "                    counter.update({reaction.meta[\"Reaction_ID\"]:\n",
    "                        {\n",
    "                            \"Recon_str\": recon_str,\n",
    "                            \"Numbers\": num,\n",
    "                            \"Structures\": structures,\n",
    "                            \"Number of decoys from strict\": num_strict,\n",
    "                            \"Number of decoys from random\": num_random,\n",
    "                            \"Random rules ID's\": rand_ids\n",
    "                        }\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(\"{} was occurred, number: {}\".format(e, n))\n",
    "                    continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgr41",
   "language": "python",
   "name": "cgr41"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
